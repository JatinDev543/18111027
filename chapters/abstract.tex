\begin{abstract}
 Prefetching is an important speculative technique that relies on prediction of the sequence of addresses that the processor will access in near-future. A prefetcher, based on this prediction, fetches the contents of the predicted addresses and places them in the processor's cache hierarchy. Over time, prefetchers have become more and more complex with some of the latest prefetching proposals advocating the use of machine learning. The primary goal of a prefetcher is to hide the latency of the load instructions. Although most prefetchers treat all load instructions equally, some load instructions are known to be more performance-critical and introduce bigger latency on the critical path of execution compared to the rest of the loads.
 The criticality of a load can be dependent on various factors like cycles it takes to fetch it, number of instructions that are dependent on it, etc.. In this thesis, we set out to identify the static load instructions that turn out to be performance-critical during execution and use this information in prefetching. There are two distinct phases in the proposed scheme. First, we identify and store the critical static loads at run-time. Second, we incorporate this information in an existing prefetcher in various different ways to address issues like cache pollution and memory bandwidth consumption. Our simulation-based study focuses on a contemporary three-level cache hierarchy model with the third level~(L3) being shared by multiple on-chip cores and the first two levels~(L1 and L2) being private to each core. Each core's L2 cache is equipped with the signature path-based prefetcher~(SPP). We use the set of critical loads to optimize the prefetch lookahead, prefetch degree, and other parameters of SPP. In a single-core setting, we are able to achieve an average 1\% performance improvement compared to SPP for a large subset of the SPEC CPU 2017 workload traces. Also, a low bandwidth variant of our proposal can maintain the performance of SPP while saving memory bandwidth. On multi-core configurations, we evaluate different flavors of our proposal geared toward different levels of memory bandwidth consumption and find that the low bandwidth flavor can maintain the performance of SPP while saving bandwidth. Across all configurations, we observe that the use of critical load addresses for triggering prefetches significantly reduces the number of times the prefetcher is activated. This is expected to lower the dynamic energy expended in the prefetcher.
\end{abstract}